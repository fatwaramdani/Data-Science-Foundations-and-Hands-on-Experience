#############
# CHAPTER 3 #
#############

library(ggplot2)
library(dplyr)
library(tidyr)
library(summarytools)
library(DataExplorer)
library(outliers)

# Load sample dataset
# Replace 'mtcars' with your dataset or read in your dataset using read.csv() or similar
data <- mtcars

# Figure 3.1 Summary statistics of the mtcars dataset.
# Summary Statistics
summary_stats <- summary(data)
print("Summary Statistics:")
print(summary_stats)

# Visualize Summary Statistics
data_long <- data %>%
  rownames_to_column(var = "Car") %>%
  pivot_longer(cols = -Car, names_to = "Variable", values_to = "Value")

# Figure 3.2 Box plot of variables of the mtcars dataset.
ggplot(data_long, aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal(base_size = 20) +
  labs(title = "", x = "Variable", y = "Value")

# Data Profiling
print("Data Profiling Report:")
create_report(data)

# Data Visualization
# Pairwise Scatter Plots
pairs(data, main = "Pairwise Scatter Plots")

# Figure 3.3 The correlation heatmap of variables as one of the profiling output analysis of the mtcars dataset.
# Correlation Heatmap
cor_matrix <- round(cor(data), 2)
ggplot(melt(cor_matrix), aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  theme_minimal(base_size = 20) +
  labs(title = "", x = "", y = "")

# Outlier Detection
print("Outlier Detection:")
outlier_results <- apply(data, 2, function(x) {
  outlier_test <- grubbs.test(x)
  return(list(statistic = outlier_test$statistic, p.value = outlier_test$p.value))
})
print(outlier_results)

# Highlight Outliers in Visualizations
outliers_df <- data %>%
  mutate(across(everything(), ~ifelse(. > mean(.) + 2 * sd(.) | . < mean(.) - 2 * sd(.), "Outlier", "Normal"))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Outlier_Status")


#Figure 3.4 Plot of outliers detection using Z-Score method og mpg variables of mtcars dataset.
# Load required library
library(ggplot2)

# Load the mtcars dataset
data("mtcars")

# Select a variable to analyze (e.g., 'mpg')
variable <- "mpg"

# Calculate the mean and standard deviation
mean_value <- mean(mtcars[[variable]])
std_dev <- sd(mtcars[[variable]])

# Compute the Z-Score for each value
mtcars$z_score <- (mtcars[[variable]] - mean_value) / std_dev

# Flag the outliers (threshold: Z-Score > |2|)
outlier_threshold <- 2
mtcars$is_outlier <- abs(mtcars$z_score) > outlier_threshold

# Plot the data
ggplot(mtcars, aes_string(x = "rownames(mtcars)", y = variable)) +
  geom_point(aes(color = is_outlier), size = 3) +  # Color points based on outlier status
  geom_text(aes(label = ifelse(is_outlier, rownames(mtcars), "")), 
            hjust = -0.2, vjust = -0.5, size = 3) +  # Add labels for outliers
  labs(title = paste("Outliers Detection using Z-Score Method (", variable, ")", sep = ""),
       x = "Car Model", y = variable) +
  scale_color_manual(values = c("black", "red"), name = "Outlier", 
                     labels = c("No", "Yes")) +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the rows flagged as outliers
outliers <- mtcars[mtcars$is_outlier, ]
print("Outliers:")
print(outliers)

#Figure 3.5 Plot of outliers detection using IQR method of mpg variables of mtcars dataset.
# Load necessary library
library(ggplot2)

# Load the mtcars dataset
data(mtcars)

# Calculate the Interquartile Range (IQR) for the mpg variable
mpg <- mtcars$mpg
Q1 <- quantile(mpg, 0.25) # First quartile (25th percentile)
Q3 <- quantile(mpg, 0.75) # Third quartile (75th percentile)
IQR <- Q3 - Q1            # Interquartile range

# Define the lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Identify outliers
outliers <- mpg[mpg < lower_bound | mpg > upper_bound]

# Create a new column in mtcars to label outliers
mtcars$outlier <- ifelse(mpg < lower_bound | mpg > upper_bound, "Outlier", "Normal")

# Add rownames as a column for labeling
mtcars$car <- rownames(mtcars)

# Create a scatterplot with custom colors and labels
ggplot(mtcars, aes(x = car, y = mpg)) +
  geom_point(aes(color = outlier), size = 3) +  # Points with colors based on "outlier"
  scale_color_manual(values = c("Normal" = "blue", "Outlier" = "red")) +  # Custom colors
  geom_hline(yintercept = c(lower_bound, upper_bound), linetype = "dashed", color = "blue") + # IQR bounds
  geom_text(data = subset(mtcars, outlier == "Outlier"), 
            aes(label = car), color = "black", hjust = -0.1, vjust = 0.5, size = 4) + # Labels for outliers
  labs(title = "Scatterplot of mpg with Outliers Highlighted",
       x = "Car Models",
       y = "Miles per Gallon (mpg)") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels



#Figure 3.6 Boxplot of outliers detection using IQR method of mpg variables of mtcars dataset.
# Load necessary library
library(ggplot2)

# Load the mtcars dataset
data(mtcars)

# Calculate the Interquartile Range (IQR) for the mpg variable
mpg <- mtcars$mpg
Q1 <- quantile(mpg, 0.25) # First quartile (25th percentile)
Q3 <- quantile(mpg, 0.75) # Third quartile (75th percentile)
IQR <- Q3 - Q1            # Interquartile range

# Define the lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Identify outliers
outliers <- mpg[mpg < lower_bound | mpg > upper_bound]

# Print outliers
cat("Outliers in mpg variable:\n", outliers, "\n")

# Create a boxplot to visualize the outliers
ggplot(mtcars, aes(x = "", y = mpg)) +
  geom_boxplot(outlier.colour = "red", outlier.size = 5, fill = "skyblue") +
  geom_text(data = subset(mtcars, mpg %in% outliers),
            aes(label = rownames(subset(mtcars, mpg %in% outliers)), y = mpg),
            nudge_x = 0.2, size = 5) +
  labs(title = "Boxplot of mpg with Outliers Highlighted",
       y = "Miles per Gallon (mpg)",
       x = "") +
  theme_minimal(base_size = 20)


#Figure 3.7 Plot of outliers detection using DBSCAN method of mpg variables of mtcars dataset.

# Load necessary libraries
library(dbscan)
library(ggplot2)

# Load the mtcars dataset
data(mtcars)

# Perform DBSCAN clustering with eps = 3 and minPts = 5 (these parameters can be adjusted)
dbscan_result <- dbscan(mtcars[, c("mpg", "hp")], eps = 3, minPts = 5)

# Add the DBSCAN clustering result to the mtcars dataset
mtcars$dbscan_cluster <- dbscan_result$cluster

# Create a plot
ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point(aes(color = as.factor(dbscan_cluster)), size = 3) +  # Normal points
  scale_color_manual(values = c("black", "blue", "red")) +
  theme_minimal() +
  # Highlight outliers in red
  geom_point(data = subset(mtcars, dbscan_cluster == 0), 
             aes(x = mpg, y = hp), 
             color = "red", size = 5) +
  # Label outliers
  geom_text(data = subset(mtcars, dbscan_cluster == 0), 
            aes(x = mpg, y = hp, label = rownames(mtcars)), 
            color = "red", vjust = -1) +
  labs(title = "DBSCAN Clustering of mtcars dataset with Outliers in Red",
       x = "Miles Per Gallon (mpg)",
       y = "Horsepower (hp)") +
  theme(legend.position = "none")


#Figure 3.8 Plot of outliers detection using Isolation Forest method of mpg and horse power variables of mtcars dataset.
# Load necessary libraries
library(randomForest)
library(ggplot2)

# Load the mtcars dataset
data(mtcars)

# Apply randomForest to simulate Isolation Forest method
set.seed(42)

# Fit random forest model
rf_model <- randomForest(mpg ~ ., data = mtcars, ntree = 100)

# Get the outlier scores (we use the proximity measure)
outlier_scores <- rf_model$proximity[,1]

# Define outliers (using a threshold on outlier scores)
threshold <- quantile(outlier_scores, 0.90)  # 95th percentile as threshold
outliers <- which(outlier_scores >= threshold)

# Prepare the plot
ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point(aes(color = "Non-Outlier"), size = 3) +
  geom_point(data = mtcars[outliers, ], aes(x = wt, y = mpg, color = "Outlier"), size = 4) +
  scale_color_manual(values = c("Non-Outlier" = "black", "Outlier" = "red")) +
  geom_text(data = mtcars[outliers, ], aes(x = wt, y = mpg, label = rownames(mtcars)[outliers]), 
            color = "red", hjust = 1.5, vjust = 1.5) +
  theme_minimal(base_size = 20) +
  labs(title = "Outlier Detection using Isolation Forest (Simulated)",
       x = "Miles per Gallon", y = "Horse Power") +
  theme(legend.position = "none")


#Figure 3.9 Plot of outliers detection using LOF method of mpg and horse power variables of mtcars dataset.
# Load required libraries
library(DBI)
library(ggplot2)
library(Rlof)

# Load the mtcars dataset
data(mtcars)

# Apply the Local Outlier Factor (LOF) method
lof_scores <- lof(mtcars, k = 5)  # k is the number of neighbors

# Add LOF scores to the mtcars dataset
mtcars$lof <- lof_scores

# Identify outliers (threshold for LOF score can be adjusted)
outliers <- which(mtcars$lof > 1.5)  # LOF score threshold for outliers

# Plotting
ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point(aes(color = "normal"), size = 3) +  # Plot normal points
  geom_point(data = mtcars[outliers, ], aes(x = mpg, y = hp), 
             color = "red", size = 5) +  # Plot outliers in red
  geom_text(data = mtcars[outliers, ], 
            aes(x = mpg, y = hp, label = rownames(mtcars)[outliers]), 
            color = "red", hjust = -0.2) +  # Label outliers
  labs(title = "Outliers in mtcars Dataset (LOF Method)", 
       x = "Miles per Gallon (mpg)", y = "Horsepower (hp)") +
  theme_minimal(base_size = 20) +
  theme(legend.position = "none")  # Hide legend

# Figure 3.10 The plot of outliers count per variable of the mtcars dataset.
ggplot(outliers_df, aes(x = Variable, fill = Outlier_Status)) +
  geom_bar(position = "dodge") +
  theme_minimal(base_size = 20) +
  labs(title = "", x = "Variable", y = "Count")


# Figure 3.11 Data cleaning, transformation, and standardization.
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggpubr) # For combining multiple plots

# 1. Create a synthetic dataset with missing values and irregular scales
set.seed(123)
data <- data.frame(
  ID = 1:15,
  Feature1 = c(NA, rnorm(14, mean = 50, sd = 10)), # Some missing values
  Feature2 = c(rnorm(10, mean = 100, sd = 20), rep(NA, 5)), # Missing values in the end
  Feature3 = c(rnorm(10, mean = 0, sd = 1), runif(5, -3, 3)) # No missing values
)

# ----- 2. Deletion -----
data_deleted <- data %>% drop_na()

# Plot missing values before and after deletion
plot_deleted <- ggplot(data, aes(x = ID)) +
  geom_point(aes(y = Feature1), color = "red") +
  geom_point(aes(y = Feature2), color = "blue") +
  labs(title = "Original Data with Missing Values (Red/Blue)") +
  theme_minimal(base_size = 20)

plot_deleted_after <- ggplot(data_deleted, aes(x = ID)) +
  geom_point(aes(y = Feature1), color = "red") +
  geom_point(aes(y = Feature2), color = "blue") +
  labs(title = "After Deletion of Rows with Missing Values") +
  theme_minimal(base_size = 20)

# ----- 3. Imputation -----
data_imputed <- data %>%
  mutate(
    Feature1 = ifelse(is.na(Feature1), mean(Feature1, na.rm = TRUE), Feature1),
    Feature2 = ifelse(is.na(Feature2), median(Feature2, na.rm = TRUE), Feature2)
  )

# Plot after imputation
plot_imputed <- ggplot(data_imputed, aes(x = ID)) +
  geom_point(aes(y = Feature1), color = "green") +
  geom_point(aes(y = Feature2), color = "orange") +
  labs(title = "After Imputation (Mean for Feature1, Median for Feature2)") +
  theme_minimal(base_size = 20)

# ----- 4. Data Transformation and Standardization -----
# Log transformation and standardization
data_transformed <- data_imputed %>%
  mutate(
    Feature1 = scale(log(Feature1 + 1)), # Adding 1 to avoid log(0)
    Feature2 = scale(Feature2),
    Feature3 = scale(Feature3)
  )

# Plot standardized features
plot_transformed <- ggplot(data_transformed, aes(x = ID)) +
  geom_line(aes(y = Feature1, color = "Feature1")) +
  geom_line(aes(y = Feature2, color = "Feature2")) +
  geom_line(aes(y = Feature3, color = "Feature3")) +
  labs(
    title = "After Data Transformation and Standardization",
    color = "Features"
  ) +
  theme_minimal(base_size = 20)

# ----- Combine all plots -----
combined_plot <- ggarrange(
  plot_deleted, plot_deleted_after, plot_imputed, plot_transformed,
  ncol = 2, nrow = 2
)

# Display the plot
print(combined_plot)

# Figure 3.12 Clustering for data reduction, where data points grouped into clusters with centroids.
# # Load necessary libraries
library(ggplot2)

# Generate synthetic data
set.seed(123)  # Set seed for reproducibility
n_points <- 300
x <- rbind(
  matrix(rnorm(n_points, mean = 2, sd = 0.6), ncol = 2),
  matrix(rnorm(n_points, mean = 5, sd = 0.8), ncol = 2),
  matrix(rnorm(n_points, mean = 8, sd = 0.5), ncol = 2)
)
data <- data.frame(x1 = x[, 1], x2 = x[, 2])

# Perform K-means clustering
k <- 3  # Number of clusters
kmeans_result <- kmeans(data, centers = k)

# Add cluster labels and centroids to the data frame
data$cluster <- as.factor(kmeans_result$cluster)
centroids <- as.data.frame(kmeans_result$centers)
colnames(centroids) <- c("x1", "x2")  # Rename columns to match data frame

# Plot the data and clusters
ggplot(data, aes(x = x1, y = x2, color = cluster)) +
  geom_point(size = 2, alpha = 0.6) +  # Data points
  geom_point(data = centroids, aes(x = x1, y = x2), color = "black", size = 4, shape = 8) +  # Centroids
  labs(
    title = "Clustering for Data Reduction",
    subtitle = "Data points grouped into clusters with centroids",
    x = "Feature 1",
    y = "Feature 2"
  ) +
  theme_minimal(base_size = 20) +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5))

# Figure 3.14 Illustration of PCA using iris dataset.
## Load required libraries
library(ggplot2)
library(ggrepel)

# Load the iris dataset
data(iris)

# Perform PCA on the iris dataset
pca_result <- prcomp(iris[, 1:4], center = TRUE, scale. = TRUE)

# Extract PCA components
pca_data <- data.frame(
  PC1 = pca_result$x[, 1],
  PC2 = pca_result$x[, 2],
  Species = iris$Species
)

# Create a scatter plot of the PCA results
ggplot(pca_data, aes(x = PC1, y = PC2, color = Species)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
  geom_segment(
    x = 0, y = 0, 
    xend = pca_result$rotation[1, 1] * max(pca_data$PC1),
    yend = pca_result$rotation[2, 1] * max(pca_data$PC1),
    arrow = arrow(length = unit(0.2, "cm")),
    color = "red", size = 1.2
  ) +
  geom_segment(
    x = 0, y = 0, 
    xend = pca_result$rotation[1, 2] * max(pca_data$PC2),
    yend = pca_result$rotation[2, 2] * max(pca_data$PC2),
    arrow = arrow(length = unit(0.2, "cm")),
    color = "green", size = 1.2
  ) +
  annotate("text", x = pca_result$rotation[1, 1] * max(pca_data$PC1) * 1.2,
           y = pca_result$rotation[2, 1] * max(pca_data$PC1) * 1.2,
           label = "PC1", color = "red", size = 5) +
  annotate("text", x = pca_result$rotation[1, 2] * max(pca_data$PC2) * 1.2,
           y = pca_result$rotation[2, 2] * max(pca_data$PC2) * 1.2,
           label = "PC2", color = "green", size = 5) +
  labs(
    title = "PCA of Iris Dataset",
    x = "Principal Component 1 (PC1)",
    y = "Principal Component 2 (PC2)"
  ) +
  theme_minimal(base_size = 20)



# Figure 3.15 Illustration of ​​the top terms per topic in the LDA model and document-topic distribution.
# Load required libraries
library(topicmodels)  # For LDA modeling
library(ggplot2)      # For visualization
library(tidyverse)    # For data wrangling and visualization
library(tidytext)     # For text analysis

# Simulate a sample document-term matrix
set.seed(123)
data("AssociatedPress")
dtm <- AssociatedPress[1:100, ]  # Use a subset for simplicity

# Fit an LDA model
k <- 3  # Number of topics
lda_model <- LDA(dtm, k = k, control = list(seed = 123))

# Extract topic-term matrix
topic_terms <- tidy(lda_model, matrix = "beta")

# Extract top terms per topic
top_terms <- topic_terms %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 5) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, beta, topic))

# Plot top terms per topic
ggplot(top_terms, aes(x = beta, y = term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  scale_y_reordered() +
  labs(
    title = "",
    x = "Beta (Word Importance in Topic)",
    y = "Terms"
  ) +
  theme_minimal(base_size = 20)

# Extract document-topic distribution
doc_topics <- tidy(lda_model, matrix = "gamma")

# Plot document-topic distribution for a sample of documents
sample_docs <- doc_topics %>% 
  filter(document %in% sample(unique(document), 10))

ggplot(sample_docs, aes(x = as.factor(document), y = gamma, fill = as.factor(topic))) +
  geom_col() +
  labs(
    title = "",
    x = "Document",
    y = "Gamma (Topic Proportion)"
  ) +
  theme_minimal(base_size = 20) +
  scale_fill_brewer(palette = "Set3", name = "Topic")


# Figure 3.16 t-SNE plot of iris dataset.
# Load libraries
library(Rtsne)
library(ggplot2)

# Generate sample data (e.g., Iris dataset)
data(iris)

# Prepare the data for t-SNE (remove non-numeric columns)
data_matrix <- as.matrix(iris[, -5])  # Exclude the Species column

# Remove duplicate rows
data_matrix <- unique(data_matrix)

# Perform t-SNE
tsne_result <- Rtsne(
  data_matrix,
  dims = 2,           # Reduce to 2 dimensions for visualization
  perplexity = 30,    # Recommended range: 5-50 (adjust as needed)
  theta = 0.5,        # Speed/accuracy trade-off (lower = more accurate)
  verbose = TRUE,
  max_iter = 1000     # Number of iterations
)

# Create a data frame for plotting
# Since duplicates were removed, adjust the Species labels accordingly
unique_indices <- which(!duplicated(as.matrix(iris[, -5])))
plot_data <- data.frame(
  X = tsne_result$Y[, 1],  # t-SNE dimension 1
  Y = tsne_result$Y[, 2],  # t-SNE dimension 2
  Species = iris$Species[unique_indices]  # Adjusted Species labels
)

# Plot the t-SNE result using ggplot2
ggplot(plot_data, aes(x = X, y = Y, color = Species)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(
    title = "",
    x = "t-SNE Dimension 1",
    y = "t-SNE Dimension 2"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )


# Figure 3.17 Illustration of holdout method.
# Load required libraries
library(ggplot2)
library(caret)

# Generate a sample dataset
set.seed(123)
n <- 100
data <- data.frame(
  x = rnorm(n),
  y = rnorm(n)
)

# Split data using Holdout Method (80% training, 20% testing)
set.seed(123)
holdout_split <- createDataPartition(data$y, p = 0.8, list = FALSE)
train_data_holdout <- data[holdout_split, ]
test_data_holdout <- data[-holdout_split, ]

# Create 5-fold Cross-validation
cv_folds <- trainControl(method = "cv", number = 5)

# Create Leave-One-Out Cross-Validation (LOOCV)
loocv_folds <- trainControl(method = "LOOCV")

# Function to plot the different splits
plot_splits <- function(data, holdout_split) {
  # Plot points for training (red) and testing (green) data for Holdout method
  ggplot(data) +
    geom_point(aes(x = x, y = y), color = 'blue', alpha = 0.5) +  # All data in blue
    geom_point(data = data[holdout_split, ], aes(x = x, y = y), color = 'red') +  # Holdout training data in red
    geom_point(data = data[-holdout_split, ], aes(x = x, y = y), color = 'green') +  # Holdout test data in green
    labs(title = "Holdout Method: Red (Train) and Green (Test)") +
    theme_minimal(base_size = 20)
}

# Call the function to plot
plot_splits(data, holdout_split)


# Figure 3.18 Illustration of cross-validation data slit, showing the training data in blue and testing data in red.
# Load necessary libraries
library(ggplot2)
library(caret)

# Create a sample dataset
set.seed(123)
data <- data.frame(
  x = rnorm(100),
  y = rnorm(100)
)

# Create a train-test split using 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Perform cross-validation using a simple linear model
model <- train(y ~ x, data = data, method = "lm", trControl = train_control)

# Generate cross-validation indices manually
cv_indices <- createFolds(data$y, k = 10)

# Plot the data with training and testing splits for each fold
ggplot(data) +
  geom_point(aes(x = x, y = y), color = "gray") +
  # Use the first fold for training and testing
  geom_point(data = data[cv_indices[[1]], ], aes(x = x, y = y), color = "blue") +
  geom_point(data = data[-cv_indices[[1]], ], aes(x = x, y = y), color = "red") +
  labs(title = "Cross-validation Data Split (Fold 1)",
       x = "X Value",
       y = "Y Value") +
  theme_minimal(base_size = 20) +
  theme(legend.position = "none")


# Figure 3.19 LOO-CV plot showing the LOO-CV process with different colors representing each split.
# Load necessary libraries
library(ggplot2)

# Simulate a dataset for illustration (you can replace this with your own dataset)
set.seed(123)
data <- data.frame(
  x = rnorm(30),  # 30 random data points
  y = rnorm(30)   # Corresponding target values
)

# Function to perform Leave-One-Out Cross-Validation (LOO-CV)
loo_cv <- function(data) {
  n <- nrow(data)
  indices <- 1:n
  split_indices <- lapply(indices, function(i) setdiff(indices, i))  # List of LOO splits
  return(split_indices)
}

# Perform Leave-One-Out Cross-Validation
splits <- loo_cv(data)

# Create a data frame to store the plot data
plot_data <- data.frame(
  x = numeric(0),  # Empty vectors to store values
  y = numeric(0),
  split = integer(0)
)

# Loop over each split and store the corresponding data points for plotting
for (i in 1:length(splits)) {
  # For each split, add all data points with the current split identifier
  split_data <- data[splits[[i]], ]
  plot_data <- rbind(plot_data, data.frame(x = split_data$x, y = split_data$y, split = rep(i, nrow(split_data))))
}

# Create the plot
ggplot(plot_data, aes(x = x, y = y, color = as.factor(split))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Leave-One-Out Cross-Validation (LOO-CV)",
       x = "X Variable", y = "Y Variable",
       color = "Split") +
  theme_minimal(base_size = 20)


# Figure 3.20 Pareto Principle: 80/20 Rule.
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Create a sample dataset to demonstrate the Pareto Principle
set.seed(42)
data <- data.frame(
  Cause = paste("Cause", 1:20),
  Effect = sample(1:100, 20, replace = TRUE)
)

# Sort the data by Effect in descending order
data <- data %>%
  arrange(desc(Effect))

# Calculate cumulative percentage of effects
data <- data %>%
  mutate(CumulativeEffect = cumsum(Effect) / sum(Effect) * 100)

# Plot the Pareto Chart
ggplot(data, aes(x = reorder(Cause, -Effect), y = Effect)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(y = CumulativeEffect), color = "red", size = 1.5, group = 1) +
  geom_point(aes(y = CumulativeEffect), color = "red", size = 3) +
  scale_y_continuous(
    sec.axis = sec_axis(~ ., name = "Cumulative Percentage", labels = scales::percent)
  ) +
  labs(
    title = "",
    x = "Cause",
    y = "Effect"
  ) +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  annotate("text", x = 5, y = 95, label = "80% of the effect", color = "red", size = 5)

#Figure 3.21 Hierarchical clustering, K-Means, and Density-baed clustering using the iris dataset.
# Load necessary libraries
library(ggplot2)
library(cluster)
library(dbscan)
library(factoextra)

# Use the iris dataset
data(iris)
iris_data <- iris[, -5] # Remove the Species column for clustering

# --- Hierarchical Clustering ---
# Load necessary libraries
library(ggplot2)
library(cluster)
library(factoextra)
library(ggdendro)

# Use the iris dataset
data(iris)
iris_data <- iris[, -5] # Remove the Species column for clustering

# --- Hierarchical Clustering ---
# Compute distance matrix and hierarchical clustering
dist_matrix <- dist(iris_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")

# Cut tree into 3 clusters (optional, as you want to visualize the dendrogram)
hc_clusters <- cutree(hc, k = 3)

# Convert hierarchical clustering to dendrogram format
dendro_data <- dendro_data(hc)

# Visualize Dendrogram using ggplot
ggplot(segment(dendro_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  theme_minimal(base_size = 20) +
  labs(title = "",
       x = "Samples",
       y = "Height") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

# --- K-Means Clustering ---
# Perform K-Means with 3 clusters
set.seed(123) # For reproducibility
kmeans_result <- kmeans(iris_data, centers = 3, nstart = 25)

# Visualize K-Means Clustering
fviz_cluster(kmeans_result, 
             data = iris_data, 
             geom = "point", 
             ellipse.type = "convex", 
             main = "K-Means Clustering")

# --- Density-Based Clustering (DBSCAN) ---
# Standardize the data for DBSCAN
scaled_data <- scale(iris_data)

# Perform DBSCAN
dbscan_result <- dbscan(scaled_data, eps = 0.5, minPts = 5)

# Add cluster results to the dataset
iris_dbscan <- iris
iris_dbscan$Cluster <- as.factor(dbscan_result$cluster)

# Visualize DBSCAN Clustering
ggplot(iris_dbscan, aes(x = Petal.Length, y = Petal.Width, color = Cluster)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("gray", "red", "blue", "green", "orange")) +
  labs(title = "DBSCAN Clustering", color = "Cluster") +
  theme_minimal(base_size = 20)


#Figure 3.22 Illustration of logistic regression.
# Load necessary libraries
library(ggplot2)

# Generate sample data
set.seed(123)
x <- seq(-10, 10, length.out = 100)                      # Predictor variable
prob <- 1 / (1 + exp(-0.8 * x + 2))                     # True logistic function
outcome <- rbinom(100, size = 1, prob = prob)           # Binary response variable

data <- data.frame(x = x, outcome = outcome, prob = prob)

# Fit logistic regression model
logistic_model <- glm(outcome ~ x, family = binomial, data = data)

# Generate predictions
x_pred <- seq(-10, 10, length.out = 100)
predicted_probs <- predict(logistic_model, newdata = data.frame(x = x_pred), type = "response")

# Create a plot
ggplot(data, aes(x = x, y = outcome)) +
  geom_point(aes(color = as.factor(outcome)), size = 3, alpha = 0.8) +  # Data points
  stat_function(fun = function(x) 1 / (1 + exp(-coef(logistic_model)[1] - coef(logistic_model)[2] * x)), 
                color = "blue", size = 1.2) +                         # Logistic regression curve
  labs(title = "",
       x = "Predictor Variable (x)",
       y = "Outcome (Probability)") +
  scale_color_manual(values = c("red", "green"),
                     name = "Outcome",
                     labels = c("0", "1")) +
  theme_minimal(base_size = 20) +
  theme(plot.title = element_text(hjust = 0.5))



# Figure 3.23 Illustration of overfitting and underfitting using synthetic data and polynomial regression models. Where the linear model (blue) is expected to underfit, the quadratic model (green) is expected to be a good fit, and the cubic model (red) is expected to overfit.
# Load necessary libraries
library(ggplot2)
library(MASS)

# Generate synthetic data: a quadratic relationship with noise
set.seed(42)
n <- 100
x <- seq(-3, 3, length.out = n)
y <- 3 * x^2 + rnorm(n, mean = 0, sd = 3)

# Create a data frame for plotting
data <- data.frame(x = x, y = y)

# Fit models of different degrees
model_linear <- lm(y ~ x, data = data)        # Linear model
model_quadratic <- lm(y ~ poly(x, 2), data = data)  # Quadratic model (good fit)
model_cubic <- lm(y ~ poly(x, 3), data = data)    # Cubic model (overfitting)

# Create a ggplot
plot <- ggplot(data, aes(x = x, y = y)) +
  geom_point(color = 'black') +
  geom_smooth(method = 'lm', formula = y ~ x, color = 'blue', se = FALSE, linetype = "solid", size = 2) + # Linear model
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), color = 'green', se = FALSE, linetype = "dashed", size = 2) + # Quadratic model
  geom_smooth(method = 'lm', formula = y ~ poly(x, 3), color = 'red', se = FALSE, linetype = "dotted", size = 2) + # Cubic model
  theme_minimal() +
  labs(title = "Overfitting and Underfitting Illustration",
       x = "X",
       y = "Y")

# Print the plot
print(plot)

