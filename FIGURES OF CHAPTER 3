#############
# CHAPTER 3 #
#############

library(ggplot2)
library(dplyr)
library(tidyr)
library(summarytools)
library(DataExplorer)
library(outliers)

# Load sample dataset
# Replace 'mtcars' with your dataset or read in your dataset using read.csv() or similar
data <- mtcars

# Figure 3.1 Summary statistics of the mtcars dataset.
# Summary Statistics
summary_stats <- summary(data)
print("Summary Statistics:")
print(summary_stats)

# Visualize Summary Statistics
data_long <- data %>%
  rownames_to_column(var = "Car") %>%
  pivot_longer(cols = -Car, names_to = "Variable", values_to = "Value")

# Figure 3.2 Box plot of variables of the mtcars dataset.
ggplot(data_long, aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal(base_size = 20) +
  labs(title = "", x = "Variable", y = "Value")

# Data Profiling
print("Data Profiling Report:")
create_report(data)

# Data Visualization
# Pairwise Scatter Plots
pairs(data, main = "Pairwise Scatter Plots")

# Figure 3.3 The correlation heatmap of variables as one of the profiling output analysis of the mtcars dataset.
# Correlation Heatmap
cor_matrix <- round(cor(data), 2)
ggplot(melt(cor_matrix), aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  theme_minimal(base_size = 20) +
  labs(title = "", x = "", y = "")

# Outlier Detection
print("Outlier Detection:")
outlier_results <- apply(data, 2, function(x) {
  outlier_test <- grubbs.test(x)
  return(list(statistic = outlier_test$statistic, p.value = outlier_test$p.value))
})
print(outlier_results)

# Highlight Outliers in Visualizations
outliers_df <- data %>%
  mutate(across(everything(), ~ifelse(. > mean(.) + 2 * sd(.) | . < mean(.) - 2 * sd(.), "Outlier", "Normal"))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Outlier_Status")

# Figure 3.4 The plot of outliers count per variable of the mtcars dataset.
ggplot(outliers_df, aes(x = Variable, fill = Outlier_Status)) +
  geom_bar(position = "dodge") +
  theme_minimal(base_size = 20) +
  labs(title = "", x = "Variable", y = "Count")


# Figure 3.5 Data cleaning, transformation, and standardization.
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggpubr) # For combining multiple plots

# 1. Create a synthetic dataset with missing values and irregular scales
set.seed(123)
data <- data.frame(
  ID = 1:15,
  Feature1 = c(NA, rnorm(14, mean = 50, sd = 10)), # Some missing values
  Feature2 = c(rnorm(10, mean = 100, sd = 20), rep(NA, 5)), # Missing values in the end
  Feature3 = c(rnorm(10, mean = 0, sd = 1), runif(5, -3, 3)) # No missing values
)

# ----- 2. Deletion -----
data_deleted <- data %>% drop_na()

# Plot missing values before and after deletion
plot_deleted <- ggplot(data, aes(x = ID)) +
  geom_point(aes(y = Feature1), color = "red") +
  geom_point(aes(y = Feature2), color = "blue") +
  labs(title = "Original Data with Missing Values (Red/Blue)") +
  theme_minimal(base_size = 20)

plot_deleted_after <- ggplot(data_deleted, aes(x = ID)) +
  geom_point(aes(y = Feature1), color = "red") +
  geom_point(aes(y = Feature2), color = "blue") +
  labs(title = "After Deletion of Rows with Missing Values") +
  theme_minimal(base_size = 20)

# ----- 3. Imputation -----
data_imputed <- data %>%
  mutate(
    Feature1 = ifelse(is.na(Feature1), mean(Feature1, na.rm = TRUE), Feature1),
    Feature2 = ifelse(is.na(Feature2), median(Feature2, na.rm = TRUE), Feature2)
  )

# Plot after imputation
plot_imputed <- ggplot(data_imputed, aes(x = ID)) +
  geom_point(aes(y = Feature1), color = "green") +
  geom_point(aes(y = Feature2), color = "orange") +
  labs(title = "After Imputation (Mean for Feature1, Median for Feature2)") +
  theme_minimal(base_size = 20)

# ----- 4. Data Transformation and Standardization -----
# Log transformation and standardization
data_transformed <- data_imputed %>%
  mutate(
    Feature1 = scale(log(Feature1 + 1)), # Adding 1 to avoid log(0)
    Feature2 = scale(Feature2),
    Feature3 = scale(Feature3)
  )

# Plot standardized features
plot_transformed <- ggplot(data_transformed, aes(x = ID)) +
  geom_line(aes(y = Feature1, color = "Feature1")) +
  geom_line(aes(y = Feature2, color = "Feature2")) +
  geom_line(aes(y = Feature3, color = "Feature3")) +
  labs(
    title = "After Data Transformation and Standardization",
    color = "Features"
  ) +
  theme_minimal(base_size = 20)

# ----- Combine all plots -----
combined_plot <- ggarrange(
  plot_deleted, plot_deleted_after, plot_imputed, plot_transformed,
  ncol = 2, nrow = 2
)

# Display the plot
print(combined_plot)

# Figure 3.6 Clustering for data reduction, where data points grouped into clusters with centroids.
# # Load necessary libraries
library(ggplot2)

# Generate synthetic data
set.seed(123)  # Set seed for reproducibility
n_points <- 300
x <- rbind(
  matrix(rnorm(n_points, mean = 2, sd = 0.6), ncol = 2),
  matrix(rnorm(n_points, mean = 5, sd = 0.8), ncol = 2),
  matrix(rnorm(n_points, mean = 8, sd = 0.5), ncol = 2)
)
data <- data.frame(x1 = x[, 1], x2 = x[, 2])

# Perform K-means clustering
k <- 3  # Number of clusters
kmeans_result <- kmeans(data, centers = k)

# Add cluster labels and centroids to the data frame
data$cluster <- as.factor(kmeans_result$cluster)
centroids <- as.data.frame(kmeans_result$centers)
colnames(centroids) <- c("x1", "x2")  # Rename columns to match data frame

# Plot the data and clusters
ggplot(data, aes(x = x1, y = x2, color = cluster)) +
  geom_point(size = 2, alpha = 0.6) +  # Data points
  geom_point(data = centroids, aes(x = x1, y = x2), color = "black", size = 4, shape = 8) +  # Centroids
  labs(
    title = "Clustering for Data Reduction",
    subtitle = "Data points grouped into clusters with centroids",
    x = "Feature 1",
    y = "Feature 2"
  ) +
  theme_minimal(base_size = 20) +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5))

# Figure 3.8 Illustration of PCA.
## Load required libraries
library(ggplot2)
library(ggrepel)

# Generate a simple dataset
set.seed(42)
data <- data.frame(
  x = rnorm(100, mean = 5, sd = 2),
  y = rnorm(100, mean = 3, sd = 1)
)

# Perform PCA
pca_result <- prcomp(data, center = TRUE, scale. = TRUE)

# Extract PCA components
pca_data <- data.frame(
  PC1 = pca_result$x[, 1],
  PC2 = pca_result$x[, 2]
)

# Create a scatter plot of the PCA results
ggplot(pca_data, aes(x = PC1, y = PC2)) +
  geom_point(color = "blue", alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
  geom_segment(
    x = 0, y = 0, 
    xend = pca_result$rotation[1, 1] * max(pca_data$PC1),
    yend = pca_result$rotation[2, 1] * max(pca_data$PC1),
    arrow = arrow(length = unit(0.2, "cm")),
    color = "red", size = 1.2
  ) +
  geom_segment(
    x = 0, y = 0, 
    xend = pca_result$rotation[1, 2] * max(pca_data$PC2),
    yend = pca_result$rotation[2, 2] * max(pca_data$PC2),
    arrow = arrow(length = unit(0.2, "cm")),
    color = "green", size = 1.2
  ) +
  annotate("text", x = pca_result$rotation[1, 1] * max(pca_data$PC1) * 1.2,
           y = pca_result$rotation[2, 1] * max(pca_data$PC1) * 1.2,
           label = "PC1", color = "red", size = 5) +
  annotate("text", x = pca_result$rotation[1, 2] * max(pca_data$PC2) * 1.2,
           y = pca_result$rotation[2, 2] * max(pca_data$PC2) * 1.2,
           label = "PC2", color = "green", size = 5) +
  labs(
    title = "",
    x = "Principal Component 1 (PC1)",
    y = "Principal Component 2 (PC2)"
  ) +
  theme_minimal(base_size = 20)


# Figure 3.9 Illustration of ​​the top terms per topic in the LDA model and document-topic distribution.
# Load required libraries
library(topicmodels)  # For LDA modeling
library(ggplot2)      # For visualization
library(tidyverse)    # For data wrangling and visualization
library(tidytext)     # For text analysis

# Simulate a sample document-term matrix
set.seed(123)
data("AssociatedPress")
dtm <- AssociatedPress[1:100, ]  # Use a subset for simplicity

# Fit an LDA model
k <- 3  # Number of topics
lda_model <- LDA(dtm, k = k, control = list(seed = 123))

# Extract topic-term matrix
topic_terms <- tidy(lda_model, matrix = "beta")

# Extract top terms per topic
top_terms <- topic_terms %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 5) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, beta, topic))

# Plot top terms per topic
ggplot(top_terms, aes(x = beta, y = term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  scale_y_reordered() +
  labs(
    title = "",
    x = "Beta (Word Importance in Topic)",
    y = "Terms"
  ) +
  theme_minimal(base_size = 20)

# Extract document-topic distribution
doc_topics <- tidy(lda_model, matrix = "gamma")

# Plot document-topic distribution for a sample of documents
sample_docs <- doc_topics %>% 
  filter(document %in% sample(unique(document), 10))

ggplot(sample_docs, aes(x = as.factor(document), y = gamma, fill = as.factor(topic))) +
  geom_col() +
  labs(
    title = "",
    x = "Document",
    y = "Gamma (Topic Proportion)"
  ) +
  theme_minimal(base_size = 20) +
  scale_fill_brewer(palette = "Set3", name = "Topic")


# Figure 3.10 t-SNE plot of iris dataset.
# Load libraries
library(Rtsne)
library(ggplot2)

# Generate sample data (e.g., Iris dataset)
data(iris)

# Prepare the data for t-SNE (remove non-numeric columns)
data_matrix <- as.matrix(iris[, -5])  # Exclude the Species column

# Remove duplicate rows
data_matrix <- unique(data_matrix)

# Perform t-SNE
tsne_result <- Rtsne(
  data_matrix,
  dims = 2,           # Reduce to 2 dimensions for visualization
  perplexity = 30,    # Recommended range: 5-50 (adjust as needed)
  theta = 0.5,        # Speed/accuracy trade-off (lower = more accurate)
  verbose = TRUE,
  max_iter = 1000     # Number of iterations
)

# Create a data frame for plotting
# Since duplicates were removed, adjust the Species labels accordingly
unique_indices <- which(!duplicated(as.matrix(iris[, -5])))
plot_data <- data.frame(
  X = tsne_result$Y[, 1],  # t-SNE dimension 1
  Y = tsne_result$Y[, 2],  # t-SNE dimension 2
  Species = iris$Species[unique_indices]  # Adjusted Species labels
)

# Plot the t-SNE result using ggplot2
ggplot(plot_data, aes(x = X, y = Y, color = Species)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(
    title = "",
    x = "t-SNE Dimension 1",
    y = "t-SNE Dimension 2"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )


# Figure 3.11 Illustration of holdout method.
# Load required libraries
library(ggplot2)
library(caret)

# Generate a sample dataset
set.seed(123)
n <- 100
data <- data.frame(
  x = rnorm(n),
  y = rnorm(n)
)

# Split data using Holdout Method (80% training, 20% testing)
set.seed(123)
holdout_split <- createDataPartition(data$y, p = 0.8, list = FALSE)
train_data_holdout <- data[holdout_split, ]
test_data_holdout <- data[-holdout_split, ]

# Create 5-fold Cross-validation
cv_folds <- trainControl(method = "cv", number = 5)

# Create Leave-One-Out Cross-Validation (LOOCV)
loocv_folds <- trainControl(method = "LOOCV")

# Function to plot the different splits
plot_splits <- function(data, holdout_split) {
  # Plot points for training (red) and testing (green) data for Holdout method
  ggplot(data) +
    geom_point(aes(x = x, y = y), color = 'blue', alpha = 0.5) +  # All data in blue
    geom_point(data = data[holdout_split, ], aes(x = x, y = y), color = 'red') +  # Holdout training data in red
    geom_point(data = data[-holdout_split, ], aes(x = x, y = y), color = 'green') +  # Holdout test data in green
    labs(title = "Holdout Method: Red (Train) and Green (Test)") +
    theme_minimal(base_size = 20)
}

# Call the function to plot
plot_splits(data, holdout_split)


# Figure 3.12 Illustration of cross-validation data slit, showing the training data in blue and testing data in red.
# Load necessary libraries
library(ggplot2)
library(caret)

# Create a sample dataset
set.seed(123)
data <- data.frame(
  x = rnorm(100),
  y = rnorm(100)
)

# Create a train-test split using 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Perform cross-validation using a simple linear model
model <- train(y ~ x, data = data, method = "lm", trControl = train_control)

# Generate cross-validation indices manually
cv_indices <- createFolds(data$y, k = 10)

# Plot the data with training and testing splits for each fold
ggplot(data) +
  geom_point(aes(x = x, y = y), color = "gray") +
  # Use the first fold for training and testing
  geom_point(data = data[cv_indices[[1]], ], aes(x = x, y = y), color = "blue") +
  geom_point(data = data[-cv_indices[[1]], ], aes(x = x, y = y), color = "red") +
  labs(title = "Cross-validation Data Split (Fold 1)",
       x = "X Value",
       y = "Y Value") +
  theme_minimal(base_size = 20) +
  theme(legend.position = "none")


# Figure 3.13 LOO-CV plot showing the LOO-CV process with different colors representing each split.
# Load necessary libraries
library(ggplot2)

# Simulate a dataset for illustration (you can replace this with your own dataset)
set.seed(123)
data <- data.frame(
  x = rnorm(30),  # 30 random data points
  y = rnorm(30)   # Corresponding target values
)

# Function to perform Leave-One-Out Cross-Validation (LOO-CV)
loo_cv <- function(data) {
  n <- nrow(data)
  indices <- 1:n
  split_indices <- lapply(indices, function(i) setdiff(indices, i))  # List of LOO splits
  return(split_indices)
}

# Perform Leave-One-Out Cross-Validation
splits <- loo_cv(data)

# Create a data frame to store the plot data
plot_data <- data.frame(
  x = numeric(0),  # Empty vectors to store values
  y = numeric(0),
  split = integer(0)
)

# Loop over each split and store the corresponding data points for plotting
for (i in 1:length(splits)) {
  # For each split, add all data points with the current split identifier
  split_data <- data[splits[[i]], ]
  plot_data <- rbind(plot_data, data.frame(x = split_data$x, y = split_data$y, split = rep(i, nrow(split_data))))
}

# Create the plot
ggplot(plot_data, aes(x = x, y = y, color = as.factor(split))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Leave-One-Out Cross-Validation (LOO-CV)",
       x = "X Variable", y = "Y Variable",
       color = "Split") +
  theme_minimal(base_size = 20)


# Figure 3.14 Pareto Principle: 80/20 Rule.
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Create a sample dataset to demonstrate the Pareto Principle
set.seed(42)
data <- data.frame(
  Cause = paste("Cause", 1:20),
  Effect = sample(1:100, 20, replace = TRUE)
)

# Sort the data by Effect in descending order
data <- data %>%
  arrange(desc(Effect))

# Calculate cumulative percentage of effects
data <- data %>%
  mutate(CumulativeEffect = cumsum(Effect) / sum(Effect) * 100)

# Plot the Pareto Chart
ggplot(data, aes(x = reorder(Cause, -Effect), y = Effect)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(y = CumulativeEffect), color = "red", size = 1.5, group = 1) +
  geom_point(aes(y = CumulativeEffect), color = "red", size = 3) +
  scale_y_continuous(
    sec.axis = sec_axis(~ ., name = "Cumulative Percentage", labels = scales::percent)
  ) +
  labs(
    title = "",
    x = "Cause",
    y = "Effect"
  ) +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  annotate("text", x = 5, y = 95, label = "80% of the effect", color = "red", size = 5)


