#############
# CHAPTER 6 #
#############

#Figure 6.4 Decision tree of iris dataset.
# Load required libraries
library(randomForest)
library(ggplot2)
library(rpart.plot) # For visualizing individual decision trees

# Load the iris dataset
data(iris)

# Set seed for reproducibility
set.seed(123)

# Train a Random Forest model
rf_model <- randomForest(Species ~ ., data = iris, ntree = 100, importance = TRUE)

# Print the model summary
print(rf_model)

# Plot the model's error rates
plot(rf_model, main = "Error Rates of Random Forest Model")

# Visualize variable importance
importance_df <- as.data.frame(importance(rf_model))
importance_df$Variable <- rownames(importance_df)

# Create a ggplot for variable importance
ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (Mean Decrease Gini)",
       x = "Variables",
       y = "Importance") +
  theme_minimal(base_size = 20)

# Visualize one of the trees from the random forest
# Extracting and plotting the first tree
tree <- getTree(rf_model, k = 1, labelVar = TRUE)
tree_model <- rpart::rpart(as.formula(paste(colnames(tree)[2], "~", paste(colnames(tree)[-1], collapse = "+"))),
                           data = iris, method = "class")
rpart.plot(tree_model)

Figure 6.5 Error rates and variable of importance of Random Forest model using iris dataset.
# Load required libraries
library(randomForest)
library(ggplot2)
library(rpart.plot) # For visualizing individual decision trees

# Load the iris dataset
data(iris)

# Set seed for reproducibility
set.seed(123)

# Train a Random Forest model
rf_model <- randomForest(Species ~ ., data = iris, ntree = 100, importance = TRUE)

# Print the model summary
print(rf_model)

# Plot the model's error rates
plot(rf_model, main = "Error Rates of Random Forest Model")

# Visualize variable importance
importance_df <- as.data.frame(importance(rf_model))
importance_df$Variable <- rownames(importance_df)

# Create a ggplot for variable importance
ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "",
       x = "Variables",
       y = "Importance") +
  theme_minimal(base_size = 20)

# Visualize one of the trees from the random forest
# Extracting and plotting the first tree
tree <- getTree(rf_model, k = 1, labelVar = TRUE)
tree_model <- rpart::rpart(as.formula(paste(colnames(tree)[2], "~", paste(colnames(tree)[-1], collapse = "+"))),
                           data = iris, method = "class")
rpart.plot(tree_model)

#Figure 6.6 CART model.
# Load necessary libraries
library(rpart)        # For building the CART model
library(rpart.plot)   # For visualizing the tree

# Create a sample dataset
set.seed(123) # Set seed for reproducibility
data <- data.frame(
  Age = sample(18:70, 100, replace = TRUE),
  Income = sample(20000:100000, 100, replace = TRUE),
  Purchased = sample(c("Yes", "No"), 100, replace = TRUE)
)

# Convert target variable to a factor
data$Purchased <- as.factor(data$Purchased)

# Build the CART model
cart_model <- rpart(Purchased ~ Age + Income, data = data, method = "class")

# Plot the tree (simple and compact)
rpart.plot(
  cart_model, 
  type = 1,      # Compact layout with splits labeled
  extra = 0,     # Show only the decision split labels
)


#Figure 6.7 SVM decision boundary.
# Install necessary package if not already installed
#if (!require("e1071")) install.packages("e1071", dependencies = TRUE)

# Load the e1071 library
library(e1071)

# Simulate some data
set.seed(123)  # For reproducibility
n <- 100  # Number of points per class
x1 <- rbind(
  cbind(rnorm(n, mean = 2), rnorm(n, mean = 2)),
  cbind(rnorm(n, mean = 5), rnorm(n, mean = 5))
)
y <- as.factor(c(rep(1, n), rep(-1, n)))  # Labels for two classes

# Combine data into a data frame
data <- data.frame(x = x1, y = y)

# Train an SVM model with a linear kernel
svm_model <- svm(y ~ ., data = data, kernel = "linear", cost = 1, scale = FALSE)

# Plot the data points
plot(data$x.1, data$x.2, col = as.numeric(data$y) + 1, pch = 19, xlab = "X1", ylab = "X2")
title("SVM Decision Boundary and Support Vectors")

# Add the decision boundary
w <- t(svm_model$coefs) %*% svm_model$SV  # Calculate weights
b <- -svm_model$rho  # Intercept

# Function for decision boundary
abline(a = -b / w[2], b = -w[1] / w[2], col = "blue", lwd = 2)  # Decision boundary

# Add margins
abline(a = (-b + 1) / w[2], b = -w[1] / w[2], col = "red", lty = 2)  # Positive margin
abline(a = (-b - 1) / w[2], b = -w[1] / w[2], col = "red", lty = 2)  # Negative margin

# Highlight support vectors
points(svm_model$SV, col = "purple", pch = 5, cex = 1.5)
legend("topleft", legend = c("Class 1", "Class -1", "Support Vectors"), 
       col = c(2, 3, "purple"), pch = c(19, 19, 5))


#Figure 6.8 Illustration of naive bayes.
# Load necessary libraries
library(ggplot2)
library(e1071)

# Set seed for reproducibility
set.seed(123)

# Generate synthetic data
n <- 200
class1 <- data.frame(x = rnorm(n, mean = 3, sd = 1), y = rnorm(n, mean = 3, sd = 1), class = 'Class 1')
class2 <- data.frame(x = rnorm(n, mean = 7, sd = 1), y = rnorm(n, mean = 7, sd = 1), class = 'Class 2')

# Combine the data
data <- rbind(class1, class2)
data$class <- as.factor(data$class)

# Train a Naive Bayes classifier
model <- naiveBayes(class ~ ., data = data)

# Generate grid for decision boundary
x_grid <- seq(min(data$x) - 1, max(data$x) + 1, length = 100)
y_grid <- seq(min(data$y) - 1, max(data$y) + 1, length = 100)
grid <- expand.grid(x = x_grid, y = y_grid)

# Predict class probabilities for the grid
grid$prob <- predict(model, grid, type = "raw")[, 1]
grid$class <- predict(model, grid)

# Plot the data points and decision boundary
ggplot(data, aes(x = x, y = y, color = class)) +
  geom_point(size = 2) +
  geom_contour(data = grid, aes(z = prob), breaks = 0.5, color = "black") +
  geom_point(data = grid, aes(x = x, y = y, fill = class), alpha = 0.3, shape = 22, size = 0.5) +
  scale_color_manual(values = c("blue", "red")) +
  labs(title = "",
       x = "Feature 1",
       y = "Feature 2",
       color = "Class") +
  theme_minimal(base_size = 20)


Figure 6.9 The Neural Network architecture and its decision boundary. 
# Install and load the necessary package
#if (!require("neuralnet")) install.packages("neuralnet", dependencies = TRUE)
library(neuralnet)

# Create the XOR dataset
xor_data <- data.frame(
  Input1 = c(0, 0, 1, 1),
  Input2 = c(0, 1, 0, 1),
  Output = c(0, 1, 1, 0)
)

# View the dataset
print("XOR Dataset:")
print(xor_data)

# Train the Neural Network
set.seed(123)  # For reproducibility
nn_model <- neuralnet(
  Output ~ Input1 + Input2,  # Formula for the model
  data = xor_data,          # Training dataset
  hidden = c(2),            # One hidden layer with 2 neurons
  linear.output = FALSE,    # Use logistic activation (for binary classification)
  threshold = 0.01          # Stopping criteria
)

# Print summary of the model
print("Neural Network Model Summary:")
print(nn_model)

# Plot the Neural Network
plot(nn_model, rep = "best", show.weights = TRUE)  # 'rep' specifies which repetition to plot if multiple

# Generate a grid of points to visualize decision boundaries
grid_points <- expand.grid(
  Input1 = seq(0, 1, length.out = 100),
  Input2 = seq(0, 1, length.out = 100)
)

# Predict outputs for the grid
grid_points$Prediction <- compute(nn_model, grid_points[, c("Input1", "Input2")])$net.result
grid_points$Prediction <- ifelse(grid_points$Prediction > 0.5, 1, 0)  # Threshold for binary classification

# Plot decision boundaries with ggplot2
ggplot() +
  geom_tile(data = grid_points, aes(x = Input1, y = Input2, fill = factor(Prediction)), alpha = 0.5) +
  geom_point(data = xor_data, aes(x = Input1, y = Input2, color = factor(Output)), size = 3) +
  scale_fill_manual(values = c("0" = "red", "1" = "blue"), name = "Prediction") +
  scale_color_manual(values = c("0" = "darkred", "1" = "darkblue"), name = "Actual") +
  labs(title = "", x = "Input1", y = "Input2") +
  theme_minimal(base_size = 20)


#Figure 6.10 The KNN decision boundary of synthetic 2D dataset.
# Load necessary libraries
library(ggplot2)
library(class) # For k-NN

# Generate sample data
set.seed(42)
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- as.factor(ifelse(x1 + x2 > 0, 1, 0))  # Class labels

# Create a data frame
data <- data.frame(x1, x2, y)

# Create a grid of points for the decision boundary
x1_range <- seq(min(x1) - 1, max(x1) + 1, length.out = 100)
x2_range <- seq(min(x2) - 1, max(x2) + 1, length.out = 100)
grid <- expand.grid(x1 = x1_range, x2 = x2_range)

# Apply the k-NN algorithm for prediction
k <- 5  # Set k value for k-NN
predicted_class <- knn(data[, c("x1", "x2")], grid, y, k)

# Convert the predicted class to a factor
grid$y <- factor(predicted_class, levels = c(0, 1))

# Plot the decision boundary and data points with a legend
ggplot() +
  geom_tile(data = grid, aes(x = x1, y = x2, fill = y), alpha = 0.3) +
  geom_point(data = data, aes(x = x1, y = x2, color = y), size = 3) +
  scale_fill_manual(values = c("red", "blue"), labels = c("Class 0", "Class 1")) +
  scale_color_manual(values = c("red", "blue"), labels = c("Class 0", "Class 1")) +
  labs(title = paste("k-NN Algorithm with k =", k),
       x = "x1", y = "x2",
       fill = "Class", color = "Class") +
  theme_minimal(base_size = 20) +
  theme(legend.position = "right")


#Figure 6.11 The the CNN architecture.
# Load the DiagrammeR library
if (!requireNamespace("DiagrammeR", quietly = TRUE)) {
  install.packages("DiagrammeR")
}
library(DiagrammeR)

# Define the CNN architecture in DOT language
cnn_diagram <- "
digraph CNN {
  rankdir=LR;  // Set left-to-right layout

  // Define node shapes
  node [shape=box, style=rounded, fontname=Helvetica, fontsize=40];

  // Input layer
  Input [label=\"Input Layer\\n(e.g., 32x32x3)\", shape=box];

  // Convolutional and pooling layers
  C1 [label=\"Conv Layer 1\\n(3x3x16)\", shape=box];
  P1 [label=\"Pooling Layer 1\\n(2x2)\", shape=box];
  C2 [label=\"Conv Layer 2\\n(3x3x32)\", shape=box];
  P2 [label=\"Pooling Layer 2\\n(2x2)\", shape=box];

  // Fully connected and output layers
  FC1 [label=\"Fully Connected Layer\\n(128 units)\", shape=box];
  Output [label=\"Output Layer\\n(Softmax)\", shape=box];

  // Connect the nodes
  Input -> C1 -> P1 -> C2 -> P2 -> FC1 -> Output;
}
"

# Render the CNN diagram
grViz(cnn_diagram)

#Figure 6.12 GBM of Boston dataset.
# Install and load required packages
#install.packages("gbm")
#install.packages("caret")  # For dataset splitting
library(gbm)
library(caret)

# Load a sample dataset (e.g., Boston housing data)
data("Boston", package = "MASS")

# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
trainIndex <- createDataPartition(Boston$medv, p = 0.8, list = FALSE)
trainData <- Boston[trainIndex, ]
testData <- Boston[-trainIndex, ]

# Fit a Gradient Boosting Model
gbm_model <- gbm(medv ~ ., 
                 data = trainData, 
                 distribution = "gaussian", 
                 n.trees = 100, 
                 shrinkage = 0.01, 
                 interaction.depth = 3)

# Plot the model's performance
# Plot the relative influence of the predictors
summary(gbm_model)

# Plot the Partial Dependence Plots (PDP) for a specific variable, e.g., "lstat"
plot(gbm_model, i.var = "lstat")

# Make predictions on the test set
predictions <- predict(gbm_model, testData, n.trees = 100)

# Evaluate the model (e.g., RMSE)
rmse <- sqrt(mean((predictions - testData$medv)^2))
cat("RMSE on test data:", rmse, "\n")

#Figure 6.13 Illustration of GMM.
# Load required packages
if (!require("mclust")) install.packages("mclust", dependencies = TRUE)
library(mclust)
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE)
library(ggplot2)

# Generate synthetic data
set.seed(42)
n1 <- 200  # Number of points in cluster 1
n2 <- 200  # Number of points in cluster 2
n3 <- 200  # Number of points in cluster 3

data <- rbind(
  cbind(rnorm(n1, mean = 2, sd = 0.8), rnorm(n1, mean = 2, sd = 0.8)),
  cbind(rnorm(n2, mean = -2, sd = 0.8), rnorm(n2, mean = 2, sd = 0.8)),
  cbind(rnorm(n3, mean = 0, sd = 0.5), rnorm(n3, mean = -2, sd = 0.5))
)
data <- as.data.frame(data)
colnames(data) <- c("x", "y")

# Fit a Gaussian Mixture Model using mclust
model <- Mclust(data)

# Extract cluster assignment and probabilities
data$cluster <- as.factor(model$classification)
data$probability <- apply(model$z, 1, max)

# Plot the data and GMM contours
ggplot(data, aes(x = x, y = y)) +
  geom_point(aes(color = cluster, alpha = probability), size = 2) +
  scale_color_manual(values = c("#1b9e77", "#d95f02", "#7570b3")) +
  scale_alpha_continuous(range = c(0.4, 1)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", bins = 15, alpha = 0.3) +
  scale_fill_viridis_c() +
  labs(
    title = "",
    x = "X-axis",
    y = "Y-axis",
    color = "Cluster"
  ) +
  theme_minimal(base_size = 20)


# Load necessary library
library(ggplot2)

# Set random seed for reproducibility
set.seed(123)

# Generate synthetic data for three classes
n <- 50  # Number of points per class
class1 <- data.frame(x = rnorm(n, mean = 2, sd = 1),
                     y = rnorm(n, mean = 4, sd = 1),
                     class = "Class 1")
class2 <- data.frame(x = rnorm(n, mean = 6, sd = 1),
                     y = rnorm(n, mean = 2, sd = 1),
                     class = "Class 2")
class3 <- data.frame(x = rnorm(n, mean = 8, sd = 1.5),
                     y = rnorm(n, mean = 6, sd = 1.5),
                     class = "Class 3")

data <- rbind(class1, class2, class3)

# Calculate mean centers for each class
centers <- aggregate(. ~ class, data, mean)

# Function to classify a new point based on minimum distance
classify_point <- function(point, centers) {
  distances <- apply(centers[, c("x", "y")], 1, function(center) {
    sqrt(sum((point - center)^2))
  })
  centers$class[which.min(distances)]
}

# Generate a grid of points to classify
x_seq <- seq(min(data$x) - 1, max(data$x) + 1, length.out = 200)
y_seq <- seq(min(data$y) - 1, max(data$y) + 1, length.out = 200)
grid <- expand.grid(x = x_seq, y = y_seq)
grid$class <- apply(grid, 1, function(row) classify_point(row, centers))

# Plot the data and classification boundaries
ggplot() +
  geom_tile(data = grid, aes(x = x, y = y, fill = class), alpha = 0.3) +
  geom_point(data = data, aes(x = x, y = y, color = class), size = 2) +
  geom_point(data = centers, aes(x = x, y = y, fill = class), size = 5, shape = 21, color = "black") +
  labs(title = "", x = "Feature 1", y = "Feature 2") +
  theme_minimal(base_size = 20) +
  scale_fill_manual(values = c("Class 1" = "blue", "Class 2" = "green", "Class 3" = "red")) +
  scale_color_manual(values = c("Class 1" = "blue", "Class 2" = "green", "Class 3" = "red"))


#Figure 6.14 The illustration of Minimum Distance algorithm.

# Load necessary library
library(ggplot2)

# Set random seed for reproducibility
set.seed(123)

# Generate synthetic data for three classes
n <- 50  # Number of points per class
class1 <- data.frame(x = rnorm(n, mean = 2, sd = 1),
                     y = rnorm(n, mean = 4, sd = 1),
                     class = "Class 1")
class2 <- data.frame(x = rnorm(n, mean = 6, sd = 1),
                     y = rnorm(n, mean = 2, sd = 1),
                     class = "Class 2")
class3 <- data.frame(x = rnorm(n, mean = 8, sd = 1.5),
                     y = rnorm(n, mean = 6, sd = 1.5),
                     class = "Class 3")

data <- rbind(class1, class2, class3)

# Calculate mean and covariance matrix for each class
class_stats <- by(data, data$class, function(subset) {
  list(mean = colMeans(subset[, c("x", "y")]),
       cov = cov(subset[, c("x", "y")]))
})

# Function to calculate the likelihood of a point belonging to a class
likelihood <- function(point, mean_vec, cov_matrix) {
  d <- length(point)
  exp(-0.5 * t(point - mean_vec) %*% solve(cov_matrix) %*% (point - mean_vec)) /
    sqrt((2 * pi)^d * det(cov_matrix))
}

# Function to classify a point using Maximum Likelihood
classify_point_mle <- function(point, class_stats) {
  likelihoods <- sapply(class_stats, function(stats) {
    likelihood(point, stats$mean, stats$cov)
  })
  names(likelihoods)[which.max(likelihoods)]
}

# Generate a grid of points to classify
x_seq <- seq(min(data$x) - 1, max(data$x) + 1, length.out = 200)
y_seq <- seq(min(data$y) - 1, max(data$y) + 1, length.out = 200)
grid <- expand.grid(x = x_seq, y = y_seq)

grid$class <- apply(grid, 1, function(row) classify_point_mle(as.numeric(row), class_stats))

# Plot the data and classification boundaries
ggplot() +
  geom_tile(data = grid, aes(x = x, y = y, fill = class), alpha = 0.3) +
  geom_point(data = data, aes(x = x, y = y, color = class), size = 2) +
  labs(title = "", x = "Feature 1", y = "Feature 2") +
  theme_minimal(base_size = 20) +
  scale_fill_manual(values = c("Class 1" = "blue", "Class 2" = "green", "Class 3" = "red")) +
  scale_color_manual(values = c("Class 1" = "blue", "Class 2" = "green", "Class 3" = "red"))


#Figure 6.15 The illustration of Maximum Likelihood algorithm.
# Load necessary libraries
library(ggplot2)
library(RColorBrewer)

# Function to calculate the Spectral Angle Mapping (SAM) between two spectra
spectral_angle_mapping <- function(spectrum1, spectrum2) {
  # Normalize the spectra
  spectrum1_norm <- spectrum1 / sqrt(sum(spectrum1^2))
  spectrum2_norm <- spectrum2 / sqrt(sum(spectrum2^2))
  
  # Compute the cosine of the angle
  cosine_angle <- sum(spectrum1_norm * spectrum2_norm)
  
  # Calculate the Spectral Angle
  angle <- acos(cosine_angle)  # Result in radians
  
  return(angle)
}

# Generate example spectra (e.g., 10 bands)
set.seed(123)
n_bands <- 10
spectrum1 <- runif(n_bands, min = 0, max = 1)
spectrum2 <- runif(n_bands, min = 0, max = 1)

# Calculate the SAM between spectrum1 and spectrum2
sam_value <- spectral_angle_mapping(spectrum1, spectrum2)

# Print the result
cat("Spectral Angle (in radians):", sam_value, "\n")

# Create a plot to visualize the spectra and their SAM
df <- data.frame(
  Band = 1:n_bands,
  Spectrum1 = spectrum1,
  Spectrum2 = spectrum2
)

ggplot(df, aes(x = Band)) +
  geom_line(aes(y = Spectrum1, color = "Spectrum 1"), size = 1.5) +
  geom_line(aes(y = Spectrum2, color = "Spectrum 2"), size = 1.5) +
  scale_color_manual(values = c("blue", "red")) +
  labs(title = paste("Spectral Angle Mapping (SAM):", round(sam_value, 2), "radians"),
       x = "Band", y = "Reflectance", color = "Spectra") +
  theme_minimal() +
  theme(legend.position = "top")




# Figure 6.16 The illustration of SAM algorithm.
# Load necessary libraries
library(ggplot2)
library(reshape2)
library(scales)

# Create example data for the image spectrum and end-member spectrum
# Simulate a spectral curve (reflectance for each band)
bands <- 1:10
image_spectrum <- c(0.1, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.85, 0.6, 0.4)  # Image spectrum
endmember_spectrum <- c(0.15, 0.25, 0.45, 0.65, 0.75, 0.85, 0.95, 0.9, 0.65, 0.45)  # End-member spectrum

# Calculate spectral angle (SAM) for each band
# Spectral angle formula: SAM = arccos((sum of (Image Spectrum * End-member Spectrum)) / (norm(Image Spectrum) * norm(End-member Spectrum)))

image_norm <- sqrt(sum(image_spectrum^2))  # Norm of image spectrum
endmember_norm <- sqrt(sum(endmember_spectrum^2))  # Norm of end-member spectrum
dot_product <- sum(image_spectrum * endmember_spectrum)  # Dot product of image and end-member spectra

spectral_angle <- acos(dot_product / (image_norm * endmember_norm))  # Spectral angle in radians
spectral_angle_deg <- spectral_angle * (180 / pi)  # Convert spectral angle to degrees

# Create a data frame for plotting
data <- data.frame(
  Band = bands,
  Image_Spectrum = image_spectrum,
  Endmember_Spectrum = endmember_spectrum
)

# Reshape data for ggplot
data_melted <- melt(data, id.vars = "Band", variable.name = "Spectrum", value.name = "Reflectance")

# Plot the spectra and show the spectral angle
ggplot(data_melted, aes(x = Band, y = Reflectance, color = Spectrum)) +
  geom_line(size = 1) +
  geom_point(aes(shape = Spectrum), size = 4) +
  scale_color_manual(values = c("blue", "red")) +
  scale_shape_manual(values = c(16, 17)) +
  labs(
    title = paste(""),
    x = "Spectral Reflectance Curve (Band)",
    y = "Band Reflectance",
    caption = "Blue: Image Spectrum, Red: End-member Spectrum"
  ) +
  theme_minimal(base_size = 20) +
  theme(legend.position = "top") +
  annotate("text", x = 7, y = 0.75, label = paste("SAM: ", round(spectral_angle_deg, 2), "Â°"), color = "black", size = 5)


# Figure 6.17 The illustration of MLP architecture with 4 layers (input, hidden, hidden, output) and ReLU and Softmax activation functions.
# Load necessary libraries
library(plotly)

# Define a function to plot MLP architecture with weights, biases, and activation functions
plot_mlp <- function(layers = c(3, 5, 4, 2), activation_funcs = c("ReLU", "ReLU", "Softmax")) {
  # Layers input defines number of nodes in each layer
  # activation_funcs defines the activation functions for each layer
  
  # Create a blank plotly plot
  plot <- plot_ly(type = "scatter", mode = "markers+text", textposition = "top center", showlegend = FALSE)
  
  # Set the x and y coordinates for each layer
  layer_x <- c(1, 2, 3, 4)
  max_nodes <- max(layers)
  
  # Calculate the y positions for nodes in each layer
  node_y <- lapply(layers, function(layer_size) {
    seq(from = 1, to = max_nodes, length.out = layer_size)
  })
  
  # Add nodes for each layer
  for (i in 1:length(layers)) {
    layer_x_coord <- rep(layer_x[i], layers[i])
    layer_y_coord <- node_y[[i]]
    layer_text <- paste("Node", 1:layers[i])
    
    plot <- plot %>%
      add_trace(x = layer_x_coord, y = layer_y_coord, text = layer_text, textfont = list(size = 10), 
                marker = list(size = 10, color = 'blue'), mode = "markers+text")
  }
  
  # Add connections between layers (weights)
  for (i in 1:(length(layers)-1)) {
    for (j in 1:layers[i]) {
      for (k in 1:layers[i+1]) {
        plot <- plot %>%
          add_trace(x = c(layer_x[i], layer_x[i+1]), y = c(node_y[[i]][j], node_y[[i+1]][k]),
                    type = 'scatter', mode = 'lines', line = list(width = 1, color = 'grey'))
      }
    }
  }
  
  # Add labels for weights (connections)
  for (i in 1:(length(layers)-1)) {
    for (j in 1:layers[i]) {
      for (k in 1:layers[i+1]) {
        weight_label <- paste("w", j, k, sep = "")
        plot <- plot %>%
          add_trace(x = (layer_x[i] + layer_x[i+1]) / 2, y = (node_y[[i]][j] + node_y[[i+1]][k]) / 2, 
                    text = weight_label, textfont = list(size = 8, color = "red"), mode = "text", showlegend = FALSE)
      }
    }
  }
  
  # Add biases (bias nodes)
  bias_y <- max_nodes + 2
  for (i in 2:length(layers)) {
    for (j in 1:layers[i]) {
      bias_label <- paste("b", j, sep = "")
      plot <- plot %>%
        add_trace(x = layer_x[i], y = bias_y, text = bias_label, textfont = list(size = 8, color = "green"), 
                  mode = "text", showlegend = FALSE)
    }
  }
  
  # Add activation function labels
  for (i in 2:(length(layers)-1)) {
    plot <- plot %>%
      add_trace(x = layer_x[i], y = max_nodes + 3, text = paste("Activation: ", activation_funcs[i-1]), 
                textfont = list(size = 10, color = "purple"), mode = "text", showlegend = FALSE)
  }
  plot <- plot %>%
    add_trace(x = layer_x[length(layers)], y = max_nodes + 3, text = paste("Activation: ", activation_funcs[length(layers)-1]), 
              textfont = list(size = 10, color = "purple"), mode = "text", showlegend = FALSE)
  
  # Add labels for layers
  plot <- plot %>%
    add_trace(x = c(1), y = max_nodes + 1, text = "Input Layer", textfont = list(size = 12, color = "black"), 
              mode = "text", showlegend = FALSE) %>%
    add_trace(x = c(2), y = max_nodes + 1, text = "Hidden Layer", textfont = list(size = 12, color = "black"), 
              mode = "text", showlegend = FALSE) %>%
    add_trace(x = c(3), y = max_nodes + 1, text = "Hidden Layer", textfont = list(size = 12, color = "black"), 
              mode = "text", showlegend = FALSE) %>%
    add_trace(x = c(4), y = max_nodes + 1, text = "Output Layer", textfont = list(size = 12, color = "black"), 
              mode = "text", showlegend = FALSE)
  
  # Customize layout
  plot <- plot %>%
    layout(title = "Multi-Layer Perceptron Architecture",
           xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
           yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
           showlegend = FALSE)
  
  return(plot)
}

# Plot MLP with 4 layers (input, hidden, hidden, output) and ReLU and Softmax activation functions
plot_mlp(layers = c(3, 5, 4, 2), activation_funcs = c("ReLU", "ReLU", "Softmax"))


# Figure 6.20 The ROC Curve.
# Load required libraries
library(ggplot2)

# Create data for the plot
fpr <- seq(0, 1, length.out = 100) # False positive rate (X-axis)

# Create curves
random_classifier <- fpr # Random classifier (diagonal line)
perfect_classifier <- pmin(fpr + 0.5, 1) # Perfect classifier
better_classifier <- 0.75 * fpr^0.5 + 0.25 # Better classifier
good_classifier <- 0.5 * fpr^0.7 + 0.5 * fpr # Good classifier
worse_classifier <- 0.5 * fpr^1.3 # Worse classifier

# Combine into a data frame
roc_data <- data.frame(
  fpr = rep(fpr, 5),
  tpr = c(random_classifier, perfect_classifier, better_classifier, good_classifier, worse_classifier),
  classifier = factor(rep(c("Random classifier", "Best classifier", "Better classifier", "Good classifier", "Worse classifier"), each = length(fpr)),
                      levels = c("Best classifier", "Better classifier", "Good classifier", "Random classifier", "Worse classifier"))
)

# Create the plot
ggplot(roc_data, aes(x = fpr, y = tpr, color = classifier, linetype = classifier)) +
  geom_line(size = 1) +
  # Customize the dashed red line for the random classifier
  geom_line(data = roc_data[roc_data$classifier == "Random classifier", ],
            aes(x = fpr, y = tpr), color = "red", linetype = "dashed", size = 1) +
  # Customize labels and theme
  labs(
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Classifier",
    linetype = "Classifier",
    title = ""
  ) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(values = c("blue", "green", "orange", "red", "black")) +
  scale_linetype_manual(values = c("solid", "solid", "solid", "dashed", "solid")) +
  theme_minimal(base_size = 20)


# Figure 6.21 The Monte Carlo uncertainty testing shows the distribution of the sample means from the simulations.
# Load required library
library(ggplot2)

# Set parameters for the Monte Carlo simulation
set.seed(123) # For reproducibility
n_simulations <- 10000   # Number of simulations
true_mean <- 50          # True mean of the population
true_sd <- 10            # True standard deviation of the population
sample_size <- 30        # Sample size

# Function to perform a single simulation
simulate <- function(n, mean, sd) {
  sample <- rnorm(n, mean, sd)  # Generate a random sample
  sample_mean <- mean(sample)  # Calculate the sample mean
  return(sample_mean)
}

# Run Monte Carlo simulations
simulated_means <- replicate(n_simulations, simulate(sample_size, true_mean, true_sd))

# Calculate uncertainty metrics
mean_of_means <- mean(simulated_means)
sd_of_means <- sd(simulated_means)
lower_ci <- mean_of_means - 1.96 * sd_of_means  # 95% Confidence Interval (Lower Bound)
upper_ci <- mean_of_means + 1.96 * sd_of_means  # 95% Confidence Interval (Upper Bound)

# Print results
cat("Mean of simulated means:", mean_of_means, "\n")
cat("Standard deviation of simulated means:", sd_of_means, "\n")
cat("95% Confidence Interval:", lower_ci, "to", upper_ci, "\n")

# Plot results
# Histogram with density overlay
ggplot(data.frame(simulated_means), aes(x = simulated_means)) +
  geom_histogram(aes(y = ..density..), bins = 50, color = "black", fill = "skyblue", alpha = 0.7) +
  geom_density(color = "red", size = 1) +
  geom_vline(xintercept = mean_of_means, color = "blue", linetype = "dashed", size = 1) +
  geom_vline(xintercept = c(lower_ci, upper_ci), color = "green", linetype = "dashed", size = 0.8) +
  labs(
    title = "",
    x = "Simulated Means",
    y = "Density"
  ) +
  theme_minimal(base_size = 20)



# Figure 6.22 Illustration of moving average, where original data is shown in blue while the forecasting data is shown in red.
# Load necessary libraries
library(ggplot2)
library(zoo)

# Sample data: Random time series data
set.seed(123)
data <- data.frame(
  time = 1:100,
  value = cumsum(rnorm(100))  # Cumulative sum to simulate random walk
)

# Calculate Moving Average with a window size of 5
data$moving_avg <- rollmean(data$value, k = 5, fill = NA)

# Plot original data and moving average
ggplot(data, aes(x = time)) +
  geom_line(aes(y = value), color = "blue", size = 1, alpha = 0.7, linetype = "dashed") +
  geom_line(aes(y = moving_avg), color = "red", size = 1) +
  labs(title = "",
       x = "Time",
       y = "Value") +
  theme_minimal(base_size = 20) +
  theme(plot.title = element_text(hjust = 0.5))



# Figure 6.23 Illustration of exponential smoothing, where original data is shown in blue while the exponentially smoothed data is shown in red.
# Load necessary libraries
library(ggplot2)
library(zoo)

# Sample data: Random time series data
set.seed(123)
data <- data.frame(
  time = 1:100,
  value = cumsum(rnorm(100))  # Cumulative sum to simulate random walk
)

# Apply Exponential Smoothing (Simple Exponential Smoothing)
alpha <- 0.2  # Smoothing parameter (between 0 and 1)
data$exp_smooth <- stats::filter(data$value, filter = alpha, method = "recursive", sides = 1)

# Plot original data and exponential smoothing
ggplot(data, aes(x = time)) +
  geom_line(aes(y = value), color = "blue", size = 1, alpha = 0.7, linetype = "dashed") +  # Original data
  geom_line(aes(y = exp_smooth), color = "red", size = 1) +  # Exponentially smoothed data
  labs(title = "",
       x = "Time",
       y = "Value") +
  theme_minimal(base_size = 20) +
  theme(plot.title = element_text(hjust = 0.5))



# Figure 6.24 Illustration of regression plot for forecasting.
# Load necessary libraries
library(ggplot2)

# Generate synthetic data for the illustration
set.seed(42)
n <- 100  # number of data points
x <- 1:n  # predictor variable (e.g., time)
y <- 3 + 2 * x + rnorm(n, mean = 0, sd = 10)  # response variable with some noise

# Create a data frame
data <- data.frame(x = x, y = y)

# Fit a linear regression model
model <- lm(y ~ x, data = data)

# Extract the model summary for statistics
model_summary <- summary(model)
intercept <- coef(model)[1]
slope <- coef(model)[2]
f_value <- model_summary$fstatistic[1]
p_value <- model_summary$coefficients[2, 4]  # p-value for the slope

# Make predictions using the model
data$predicted_y <- predict(model, newdata = data)

# Create the plot
ggplot(data, aes(x = x)) +
  geom_point(aes(y = y), color = "blue", alpha = 0.6) +  # Original data points
  geom_line(aes(y = predicted_y), color = "red", size = 1) +  # Regression line
  labs(title = "",
       x = "Predictor (X)",
       y = "Response (Y)") +
  annotate("text", x = 20, y = min(data$y), label = "Data Points", color = "blue", size = 5, hjust = 0) +  # Label for blue dots
  annotate("text", x = 20, y = 100, label = "Regression Line", color = "red", size = 5, hjust = 0) +  # Label for red line
  annotate("text", x = 60, y = max(data$y), label = paste("Equation: y = ", round(intercept, 2), " + ", round(slope, 2), " * x", sep = ""),
           color = "black", size = 5) +  # Display the regression equation
  annotate("text", x = 60, y = max(data$y) - 20, label = paste("F-statistic: ", round(f_value, 2), ", p-value: ", round(p_value, 3), sep = ""),
           color = "black", size = 5) +  # Display the F-statistic and p-value
  theme_minimal(base_size = 20) +
  theme(plot.title = element_text(hjust = 0.5))


# Figure 6.25 Illustration of the ARIMA plot using synthetic data.
# Load necessary libraries
if (!require("forecast")) install.packages("forecast", dependencies = TRUE)
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE)
library(forecast)
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Generate a sample time series data
data <- ts(rnorm(100, mean = 10, sd = 2), frequency = 12)

# Fit an ARIMA model to the data
arima_model <- auto.arima(data)

# Forecast the next 20 periods with 80% and 95% confidence intervals
forecast_result <- forecast(arima_model, h = 20, level = c(80, 95))

# Convert the forecast object to a data frame for ggplot
forecast_df <- data.frame(
  Time = time(forecast_result$mean),
  Mean = as.numeric(forecast_result$mean),
  Lo80 = as.numeric(forecast_result$lower[, 1]),
  Hi80 = as.numeric(forecast_result$upper[, 1]),
  Lo95 = as.numeric(forecast_result$lower[, 2]),
  Hi95 = as.numeric(forecast_result$upper[, 2])
)

# Create the plot
ggplot() +
  # Plot the confidence intervals
  geom_ribbon(data = forecast_df, aes(x = Time, ymin = Lo95, ymax = Hi95, fill = "95% Interval"), 
              alpha = 0.2, inherit.aes = FALSE) +
  geom_ribbon(data = forecast_df, aes(x = Time, ymin = Lo80, ymax = Hi80, fill = "80% Interval"), 
              alpha = 0.4, inherit.aes = FALSE) +
  # Plot the mean forecast
  geom_line(data = forecast_df, aes(x = Time, y = Mean, color = "Mean"), size = 1) +
  # Plot the historical data
  geom_line(data = data.frame(Time = time(data), Value = as.numeric(data)), 
            aes(x = Time, y = Value), color = "black", size = 0.8) +
  # Add labels and title
  labs(
    title = "",
    x = "Time",
    y = "Value"
  ) +
  # Customize the theme
  theme_minimal(base_size = 20) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "bottom"
  ) +
  # Add a legend for the intervals and mean forecast
  scale_fill_manual(values = c("95% Interval" = "red", "80% Interval" = "orange")) +
  scale_color_manual(values = c("Mean" = "blue")) +
  guides(fill = guide_legend(title = "Confidence Intervals"), 
         color = guide_legend(title = "Forecast")) 
